# inferless.yaml
model_name: LatentSync-Deployment # 给你的部署模型起个名字
language: python
python_version: "3.10" # 应与 inferless-runtime-config.yaml 中的一致
runtime_id: "latentsync-custom-runtime" # 引用上面定义的运行时名称
handler_path: "app.py" # Inferless 的入口 Python 文件
class_name: "Handler"    # 入口文件中的类名

# input_schema_path: "input_schema.py" # Inferless 通常会自动检测 input_schema.py

# 资源配置 (非常重要，根据 LatentSync 的需求调整)
# 根据您的模型大小和性能需求，取消注释并选择合适的 GPU 类型和数量
# 初始部署时，可以从较小的 GPU 开始，例如 T4
resources:
  gpu_type: "T4"  # 例如 NVIDIA T4。其他选项如 A10G, A100 等。
  gpu_count: 1      # GPU 数量
  min_replicas: 0   # 最小实例数 (0 表示可以缩容到零以节省成本)
  max_replicas: 1   # 最大实例数 (根据负载预期调整)
  inference_timeout_s: 300 # 推理超时时间 (秒)，对于图像生成可能需要更长
  # cpu_request: "1"    # CPU 请求 (vCPU 核心数)
  # memory_request: "8Gi" # 内存请求 (例如 8Gi, 16Gi)，根据模型大小调整

# 如果模型权重需要从外部下载或很大，考虑使用 Inferless Volumes
# volumes:
#   - name: model-cache # 卷的名称
#     mount_path: /models # 挂载到容器内的路径
#     volume_id: "your-volume-id-from-inferless-ui" # 在 Inferless UI 创建卷后获取

# 环境变量 (如果你的 app.py 需要)
# environment_variables:
#   MODEL_ID: "stabilityai/stable-diffusion-xl-base-1.0" # 示例，如果模型 ID 是可配置的
#   HF_HOME: "/models/huggingface_cache" # 如果使用 volume 缓存 Hugging Face 模型

# Secrets (如果需要 API 密钥等)
# secrets:
#   HUGGING_FACE_TOKEN: "your-hf-token-secret-name-in-inferless"
